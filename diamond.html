<html>

<head>

<title>Orthogonal Projection</title>
<meta name="description" content="Orthogonal Projection">

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>

</head>


<body>


<h1>Diamond</h1>


<table>

<tr>
<td></td>
<td></td>
<td><a href="#singularvaluedecomposition">Singular Value<br />Decomposition</a></td>
<td style="text-align: center;">&rarr;</td>
<td><a href="#pseudoinverse">Pseudoinverse</a></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>↗</td>
<td></td>
<td></td>
<td></td>
<td>↘</td>
<td></td>
</tr>
<tr>
<td style="text-align: right;">↗</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>↘</td>
</tr>
<tr>
<td><a href="#gramschmidtprocess">Gram-Schmidt<br />Process</a></td>
<td>&larr;</td>
<td style="text-align: center;">&larr;</td>
<td style="text-align: center;"><a href="#orthogonalprojection">Orthogonal<br />Projection</a></td>
<td style="text-align: center;">&rarr;</td>
<td>&rarr;</td>
<td><a href="#leastsquareproblem">Least Square<br />Problem</a></td>
</tr>
<tr>
<td style="text-align: right;">↘</td>
<td></td>
<td></td>
<td style="text-align: center;">&darr;</td>
<td></td>
<td></td>
<td>↗</td>
</tr>
<tr>
<td></td>
<td></td>
<td style="text-align: center;">↘</td>
<td style="text-align: center;"><a href="#householder">Householder<br />Reflection</a></td>
<td style="text-align: center;">↗</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td style="text-align: right;">↘</td>
<td style="text-align: center;">&darr;</td>
<td>↗</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td style="text-align: center;"><a href="#qrdecomposition">QR-decomposition</a></td>
<td></td>
<td></td>
<td></td>
</tr>

</table>

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="orthogonalbasis">Orthogonal Basis</h2>

<p>
<strong>Anton, Theorem 6.3.2.</strong> If \(S=\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n\}\) is an orthogonal basis for an inner product space \(V\), and if \(\mathbf{u}\) is any vector in \(V\), then 
\[\mathbf{u}=\frac{\langle \mathbf{u}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1+\frac{\langle \mathbf{u}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2+\cdots +\frac{\langle \mathbf{u}, \mathbf{v}_n\rangle}{||\mathbf{v}_n||^2}\mathbf{v}_n.\]
</p>

<p>
<strong>Proof:</strong> Suppose that \(\mathbf{u}=a_1\mathbf{v}_1+a_2\mathbf{v}_2+\cdots +a_n\mathbf{v}_n\). Then \(\langle \mathbf{u}, \mathbf{v_i}\rangle=a_i\langle \mathbf{v}_i, \mathbf{v}_i\rangle=a_i ||\mathbf{v}_i||^2\) and \(a_i=\frac{\langle \mathbf{u}, \mathbf{v}_i\rangle}{||\mathbf{v}_i||^2}\)
</p>

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="orthogonalprojection">Orthogonal Projection</h2>

<p>
<strong>Anton, Theorem 6.3.4.</strong> Let \(W\) be a finite-dimensional subspace of an inner product space \(V\). If \(\{\mathbf{w}_1, \mathbf{w}_2, ..., \mathbf{w}_r\}\) is an orthogonal basis for \(W\), and \(\mathbf{u}\) is any vector in \(V\), then 
\[\text{proj}_{W}\mathbf{u}=\frac{\langle \mathbf{u}, \mathbf{w}_1\rangle}{||\mathbf{w}_1||^2}\mathbf{w}_1+\frac{\langle \mathbf{u}, \mathbf{w}_2\rangle}{||\mathbf{w}_2||^2}\mathbf{w}_2+\cdots +\frac{\langle \mathbf{u}, \mathbf{w}_r\rangle}{||\mathbf{w}_r||^2}\mathbf{w}_r.\]
</p>

<!-- 這個是課本原本的敘述
<p>
<strong>Anton, Theorem 6.3.4.</strong> Let \(W\) be a finite-dimensional subspace of an inner product space \(V\). If \(\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_r\}\) is an orthogonal basis for \(W\), and \(\mathbf{u}\) is any vector in \(V\), then 
\[\text{proj}_{W}\mathbf{u}=\frac{\langle \mathbf{u}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1+\frac{\langle \mathbf{u}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2+\cdots +\frac{\langle \mathbf{u}, \mathbf{v}_r\rangle}{||\mathbf{v}_r||^2}\mathbf{v}_r.\]
</p>
-->

<p>
<strong>Proof:</strong>
\[
\begin{array}{rcl}
  &amp;   &amp; \frac{\langle \mathbf{u}, \mathbf{w}_1\rangle}{||\mathbf{w}_1||^2}\mathbf{w}_1+\frac{\langle \mathbf{u}, \mathbf{w}_2\rangle}{||\mathbf{w}_2||^2}\mathbf{w}_2+\cdots +\frac{\langle \mathbf{u}, \mathbf{w}_r\rangle}{||\mathbf{w}_r||^2}\mathbf{w}_r \\
  &amp; = &amp; \frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{w}_1\rangle}{||\mathbf{w}_1||^2}\mathbf{w}_1+\frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{w}_2\rangle}{||\mathbf{w}_2||^2}\mathbf{w}_2+\cdots +\frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{w}_r\rangle}{||\mathbf{w}_r||^2}\mathbf{w}_r \\
  &amp; \stackrel{\langle \mathbf{u}-\text{proj}_{W}\mathbf{u}, \mathbf{w}_i\rangle=0}{=} &amp; \frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{w}_1\rangle}{||\mathbf{w}_1||^2}\mathbf{w}_1+\frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{w}_2\rangle}{||\mathbf{w}_2||^2}\mathbf{w}_2+\cdots +\frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{w}_r\rangle}{||\mathbf{w}_r||^2}\mathbf{w}_r \\
  &amp; \stackrel{\text{Theorem 6.3.2}}{=} &amp; \text{proj}_{W}\mathbf{u}.
\end{array}
\]
</p>

<!-- 這個是課本原本的證明
<p>
<strong>Proof:</strong>
\[
\begin{array}{rcl}
  &amp;   &amp; \frac{\langle \mathbf{u}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1+\frac{\langle \mathbf{u}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2+\cdots +\frac{\langle \mathbf{u}, \mathbf{v}_r\rangle}{||\mathbf{v}_r||^2}\mathbf{v}_r \\
  &amp; = &amp; \frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1+\frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2+\cdots +\frac{\langle \text{proj}_{W}\mathbf{u}+(\mathbf{u}-\text{proj}_{W}\mathbf{u}), \mathbf{v}_r\rangle}{||\mathbf{v}_r||^2}\mathbf{v}_r \\
  &amp; \stackrel{\langle \mathbf{u}-\text{proj}_{W}\mathbf{u}, \mathbf{v}_i\rangle=0}{=} &amp; \frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1+\frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2+\cdots +\frac{\langle \text{proj}_{W}\mathbf{u}, \mathbf{v}_r\rangle}{||\mathbf{v}_r||^2}\mathbf{v}_r \\
  &amp; \stackrel{\text{Theorem 6.3.2}}{=} &amp; \text{proj}_{W}\mathbf{u}.
\end{array}
\]
</p>
-->

<br><img width="70%" src="img/diamond/orthogonal_projection_01.jpg">

<font color="red">注意到Theorem 6.3.2的 \(\mathbf{u}\) 是在 \(V\) 中，Theorem 6.3.4的 \(\mathbf{u}\) 並不在 \(W\) 中，但兩者的公式幾乎是一樣的。</font>

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="householder">Householder Reflections</h2>

<p>See Anton Contemporary, sec.7.10.</p>

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="gramschmidtprocess">Gram-Schmidt Process</h2>

<p>Anton, p.371. To convert a basis \(\{\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_r\}\) into an orthogonal basis \(\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_r\}\), perform the following computations:
\[\mathbf{v}_i=\mathbf{u}_i-\frac{\langle \mathbf{u}_i, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2}\mathbf{v}_1-\frac{\langle \mathbf{u}_i, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2}\mathbf{v}_2-\cdots -\frac{\langle \mathbf{u}_i, \mathbf{v}_{i-1}\rangle}{||\mathbf{v}_{i-1}||^2}\mathbf{v}_{i-1}\]
</p>

<p>
<strong>Proof:</strong> \(\mathbf{v}_i=\mathbf{u}_i-\text{proj}_{\text{span}(\{\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_{i-1}\})}\mathbf{u}_i\). Then by Theorem 6.3.4.
</p>

<!-- 以前自創的Gram-Schmidt的記憶法，但不怎麼好用

<p><font color="red"><strong>Mnemonic: </strong></font></p>

<p>
Consider the following equation of matrices:
\[
\begin{pmatrix}1 & 0 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_2, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & 1 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_3, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_3, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & 1 & 0 \\ -\frac{\langle \mathbf{u}_n, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & -\frac{\langle \mathbf{u}_n, \mathbf{v}_{n-1}\rangle}{||\mathbf{v}_{n-1}||^2} & 1\end{pmatrix}
\begin{pmatrix}\mathbf{u}_1 \\ \mathbf{u}_2 \\ \mathbf{u}_3 \\ \vdots \\ \mathbf{u}_n \end{pmatrix}=
\begin{pmatrix}\mathbf{v}_1 \\ \mathbf{v}_2 \\ \mathbf{v}_3 \\ \vdots \\ \mathbf{v}_n \end{pmatrix}
\]
</p>

<p>
Note that the matrix is lower triangular and all diagonal entries are \(1\) and \((i, j)\) entry is \(-\frac{\langle \mathbf{u}_i, \mathbf{v}_j\rangle}{||\mathbf{v}_j||^2}\), where \(i > j\).
</p>

<p>
When you compute \(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n\), you should replace \(\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_{n-1}\) by \(\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_{n-1}\), respectively and dynamically.
</p>

<p>
For example, when you compute \(\mathbf{v}_2\), you should use the following equation:
\[
\begin{pmatrix}1 & 0 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_2, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & 1 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_3, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_3, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & 1 & 0 \\ -\frac{\langle \mathbf{u}_n, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & -\frac{\langle \mathbf{u}_n, \mathbf{v}_{n-1}\rangle}{||\mathbf{v}_{n-1}||^2} & 1\end{pmatrix}
\begin{pmatrix}\color{red}{\mathbf{v}_1} \\ \mathbf{u}_2 \\ \mathbf{u}_3 \\ \vdots \\ \mathbf{u}_n \end{pmatrix}=
\begin{pmatrix}\mathbf{v}_1 \\ \mathbf{v}_2 \\ \mathbf{v}_3 \\ \vdots \\ \mathbf{v}_n \end{pmatrix}
\]
</p>

<p>
When you compute \(\mathbf{v}_3\), you should use the following equation:
\[
\begin{pmatrix}1 & 0 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_2, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & 1 & 0 & \cdots & 0 & 0 \\ -\frac{\langle \mathbf{u}_3, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_3, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_{n-1}, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & 1 & 0 \\ -\frac{\langle \mathbf{u}_n, \mathbf{v}_1\rangle}{||\mathbf{v}_1||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_2\rangle}{||\mathbf{v}_2||^2} & -\frac{\langle \mathbf{u}_n, \mathbf{v}_3\rangle}{||\mathbf{v}_3||^2} & \cdots & -\frac{\langle \mathbf{u}_n, \mathbf{v}_{n-1}\rangle}{||\mathbf{v}_{n-1}||^2} & 1\end{pmatrix}
\begin{pmatrix}\color{red}{\mathbf{v}_1} \\ \color{red}{\mathbf{v}_2} \\ \mathbf{u}_3 \\ \vdots \\ \mathbf{u}_n \end{pmatrix}=
\begin{pmatrix}\mathbf{v}_1 \\ \mathbf{v}_2 \\ \mathbf{v}_3 \\ \vdots \\ \mathbf{v}_n \end{pmatrix}
\]
</p>

-->

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="qrdecomposition">\(QR\)-Decomposition</h2>

<p>
Anton, thm.6.3.7. If \(A\) is an \(m\times n\) matrix with linearly independent column vectors, then \(A\) can be factored as \[A=QR,\] where \(Q\) is an \(m\times n\) matrix with orthonormal column vectors, and \(R\) is an \(n\times n\) invertible upper triangular matrix.
</p>

<p>
<strong>Proof:</strong>
</p>

<ul>
<li>Suppose that the column vectors of \(A\) are \(\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n\). Apply the Gram-Schmidt process to transform \(\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n\) into an orthonormal set \(\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_n\). Then denote
\[A=\begin{pmatrix}\mathbf{u}_1 | \mathbf{u}_2 |\cdots |\mathbf{u}_n\end{pmatrix}\text{ and }Q=\begin{pmatrix}\mathbf{q}_1 | \mathbf{q}_2 |\cdots |\mathbf{q}_n\end{pmatrix}\]
</li>
<li>
Express \(\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n\) by  \(\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_n\) by Theorem 6.3.2,
\[
\begin{array}{ccc}
\mathbf{u}_1 &amp;=&amp; \langle \mathbf{u}_1, \mathbf{q}_1\rangle\mathbf{q}_1 + \langle \mathbf{u}_1, \mathbf{q}_2\rangle\mathbf{q}_2 + \cdots + \langle \mathbf{u}_1, \mathbf{q}_n\rangle\mathbf{q}_n \\
\mathbf{u}_2 &amp;=&amp; \langle \mathbf{u}_2, \mathbf{q}_1\rangle\mathbf{q}_1 + \langle \mathbf{u}_2, \mathbf{q}_2\rangle\mathbf{q}_2 + \cdots + \langle \mathbf{u}_2, \mathbf{q}_n\rangle\mathbf{q}_n \\
\vdots &amp; \vdots &amp; \vdots  \\
\mathbf{u}_n &amp;=&amp; \langle \mathbf{u}_n, \mathbf{q}_1\rangle\mathbf{q}_1 + \langle \mathbf{u}_n, \mathbf{q}_2\rangle\mathbf{q}_2 + \cdots + \langle \mathbf{u}_n, \mathbf{q}_n\rangle\mathbf{q}_n 
\end{array}
\]
Equivalently,
\[\begin{pmatrix}\mathbf{u}_1 | \mathbf{u}_2 |\cdots |\mathbf{u}_n\end{pmatrix}=\begin{pmatrix}\mathbf{q}_1 | \mathbf{q}_2 |\cdots |\mathbf{q}_n\end{pmatrix}\begin{pmatrix} \langle \mathbf{u}_1, \mathbf{q}_1\rangle &amp; \langle \mathbf{u}_2, \mathbf{q}_1\rangle &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_1\rangle \\ \langle \mathbf{u}_1, \mathbf{q}_2\rangle &amp; \langle \mathbf{u}_2, \mathbf{q}_2\rangle &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_2\rangle \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \langle \mathbf{u}_1, \mathbf{q}_n\rangle &amp; \langle \mathbf{u}_2, \mathbf{q}_n\rangle &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_n\rangle\end{pmatrix}.\]
Note that the \((i,j)\)-entry of the last matrix is \(\langle \mathbf{u}_j, \mathbf{q}_i\rangle\).
</li>
<li>
Since \(\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_n\) were obtained by Gram-Schmidt process, we have
\[\mathbf{q}_i=\mathbf{u}_i-\frac{\langle \mathbf{u}_i, \mathbf{q}_1\rangle}{||\mathbf{q}_1||^2}\mathbf{q}_1-\frac{\langle \mathbf{u}_i, \mathbf{q}_2\rangle}{||\mathbf{q}_2||^2}\mathbf{q}_2-\cdots -\frac{\langle \mathbf{u}_i, \mathbf{q}_{i-1}\rangle}{||\mathbf{q}_{i-1}||^2}\mathbf{q}_{i-1}.\]
Equivalently, 
\[\mathbf{u}_i=\frac{\langle \mathbf{u}_i, \mathbf{q}_1\rangle}{||\mathbf{q}_1||^2}\mathbf{q}_1+\frac{\langle \mathbf{u}_i, \mathbf{q}_2\rangle}{||\mathbf{q}_2||^2}\mathbf{q}_2+\cdots +\frac{\langle \mathbf{u}_i, \mathbf{q}_{i-1}\rangle}{||\mathbf{q}_{i-1}||^2}\mathbf{q}_{i-1}+\mathbf{q}_i\]
This follows that
\[
\langle \mathbf{u}_i, \mathbf{q}_j\rangle=0 \text{ if }i &lt; j.
\]
Therefore,
\[A=\begin{pmatrix}\mathbf{u}_1 | \mathbf{u}_2 |\cdots |\mathbf{u}_n\end{pmatrix}=\begin{pmatrix}\mathbf{q}_1 | \mathbf{q}_2 |\cdots |\mathbf{q}_n\end{pmatrix}\begin{pmatrix} \langle \mathbf{u}_1, \mathbf{q}_1\rangle &amp; \langle \mathbf{u}_2, \mathbf{q}_1\rangle &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_1\rangle \\ 0 &amp; \langle \mathbf{u}_2, \mathbf{q}_2\rangle &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_2\rangle \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; \langle \mathbf{u}_n, \mathbf{q}_n\rangle\end{pmatrix}\stackrel{\text{def.}}{=}QR.\]
</li>
<li>
Furthermore, if \(\langle \mathbf{u}_i, \mathbf{q}_i\rangle=0\) for some \(i\), 
then \[\mathbf{u}_i=\langle \mathbf{u}_i, \mathbf{q}_1\rangle \mathbf{q}_1+\langle \mathbf{u}_i, \mathbf{q}_2\rangle \mathbf{q}_2+\cdots +\langle \mathbf{u}_i, \mathbf{q}_{i-1}\rangle \mathbf{q}_{i-1}\in \text{span}(\{\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_{i-1}\})=\text{span}(\{\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_{i-1}\}).\]
Which contraries to the independency of \(\{\mathbf{u}_1, \mathbf{u}_2, ..., \mathbf{u}_n\}\).
Therefore, \(\langle \mathbf{u}_i, \mathbf{q}_i\rangle\neq 0\) for all \(i\) and \(\det{R}=\langle \mathbf{u}_1, \mathbf{q}_1\rangle\langle \mathbf{u}_2, \mathbf{q}_2\rangle\cdots \langle \mathbf{u}_n, \mathbf{q}_n\rangle\neq 0\) and \(R\) is invertible.
</li>
</ul>


<!---------------------------------->
<!---------------------------------->
<!---------------------------------->


<h2 id="leastsquareproblem">Least Squares Problem</h2>


<canvas id="myCanvas1" width="240" height="190" style="border:1px solid #d3d3d3;"></canvas>

<script>
var c = document.getElementById("myCanvas1");
var ctx = c.getContext("2d");

// x-axis
ctx.moveTo(205, 160);
ctx.lineTo(15, 160);

// y-axis
ctx.moveTo(25, 20);
ctx.lineTo(25, 175);

// line
ctx.moveTo(205, 40);
ctx.lineTo(40, 125);

ctx.stroke();

// x label
ctx.font = "30px Arial";
ctx.fillText("x", 210, 155);

// y label
ctx.font = "30px Arial";
ctx.fillText("y", 30, 20);

// actual point label
ctx.font = "15px Arial";
ctx.fillText("(x1, y1)", 30, 135);

// approximation label
ctx.font = "15px Arial";
ctx.fillText("(x1, ax1+b)", 12, 100);

// actual point
ctx.fillRect(70,115,5,5);

// approximation
ctx.fillRect(70,106,5,5);

// other approximation
ctx.fillRect(94,90,5,5);
ctx.fillRect(111,80,5,5);
ctx.fillRect(118,88,5,5);
ctx.fillRect(138,60,5,5);
ctx.fillRect(158,58,5,5);
ctx.fillRect(172,68,5,5);
ctx.fillRect(180,40,5,5);

</script>

\[
\begin{array}{ll}
                & \text{Minimize } \sqrt{[y_1-(ax_1+b)]^2+\cdots +[y_n-(ax_n+b)]^2} \\
\Leftrightarrow & \text{Minimize } ||\begin{bmatrix}y_1-(ax_1+b)\\\vdots\\y_n-(ax_n+b)\end{bmatrix}|| \\
\Leftrightarrow & \text{Minimize } ||\begin{bmatrix}y_1\\\vdots\\y_n\end{bmatrix}_{\mathbf{b}}-\begin{bmatrix}x_1&1\\\vdots&\vdots\\x_n&1\end{bmatrix}_{A}\begin{bmatrix}a\\b\end{bmatrix}_{\mathbf{x}}|| \\
\Leftrightarrow & \text{Minimize } ||\mathbf{b}-A\mathbf{x}|| \\
\end{array}
\]

<p>注意到是 \(\text{Minimize } \sqrt{[y_1-(a_1 x+b)]^2+\cdots +[y_n-(ax_n+b)]^2}\)，不是 \(\text{Minimize }|y_1-(a_1 x+b)|+\cdots +|y_n-(ax_n+b)|\) ，而且兩者並不等價，例如 \(3+4\leq 1+5\not\Leftarrow 3^2+4^2\leq 1^2+5^2\)。
</p>

<!-- 拋物線的least square
<canvas id="myCanvas2" width="240" height="190" style="border:1px solid #d3d3d3;"></canvas>

<script>
var c = document.getElementById("myCanvas2");
var ctx = c.getContext("2d");

// parabola
ctx.beginPath();
ctx.moveTo(35, 40);
ctx.bezierCurveTo(75, 160, 150, 160, 200, 40);

// x-axis
ctx.moveTo(205, 160);
ctx.lineTo(15, 160);

// y-axis
ctx.moveTo(25, 20);
ctx.lineTo(25, 175);

ctx.stroke();

// x label
ctx.font = "30px Arial";
ctx.fillText("x", 210, 155);

// y label
ctx.font = "30px Arial";
ctx.fillText("y", 30, 20);

// actual point label
ctx.font = "15px Arial";
ctx.fillText("(x1, y1)", 30, 135);

// approximation label
ctx.font = "15px Arial";
ctx.fillText("(x1, ax1+b)", 12, 100);

// actual point
ctx.fillRect(70,115,5,5);

// approximation
ctx.fillRect(70,106,5,5);

// other approximation
ctx.fillRect(94,116,5,5);
ctx.fillRect(113,133,5,5);
ctx.fillRect(118,126,5,5);
ctx.fillRect(140,108,5,5);
ctx.fillRect(159,106,5,5);
ctx.fillRect(175,96,5,5);
ctx.fillRect(180,80,5,5);

</script>
-->

<p>Anton, p.379. Given a linear system \(A\mathbf{x}=\mathbf{b}\) of \(m\) equations in \(n\) unknowns, find a vector \(\mathbf{x}\) in \(\mathbb{R}^n\) that minimizes \(||\mathbf{b}-A\mathbf{x}||\) with respect to the Euclidean inner product on \(\mathbb{R}^m\). We call such a vector, if it exists, a least squares solution of \(A\mathbf{x}=\mathbf{b}\), we call \(\mathbf{b}-A\mathbf{x}\) the least squares error vector, and we call \(||\mathbf{b}-A\mathbf{x}||\) the least squares error.
</p>

<p>There are three methods to find \(\mathbf{x}\).
</p>

<ol>

<li>Anton, p.380. \(\mathbf{x}\) is the solution of \(A\mathbf{x}=\text{proj}_{\text{CS}(A)}\mathbf{b}\).
<br><img width="70%" src="img/diamond/orthogonal_projection_02.jpg">
但這個方法不太實用，因為 \(\text{proj}_{\text{CS}(A)}\mathbf{b}\) 並不容易求，要求的話得先用Gram-Schmidt求出 \(\text{CS}(A)\) 的一組orthonormal basis。
</li>

<li>Anton, p.380. \(\mathbf{x}\) is the solution of \(A^t A \mathbf{x}=A^t \mathbf{b}\). 
<br>
<strong>Proof:</strong> 
\[
\begin{array}{cl}
& A\mathbf{x}=\text{proj}_{\text{CS}(A)}\mathbf{b} \\
\Rightarrow &amp; \mathbf{b}-A\mathbf{x}=\mathbf{b}-\text{proj}_{\text{CS}(A)}\mathbf{b} \\ 
\Rightarrow &amp; A^t(\mathbf{b}-A\mathbf{x})=A^t(\mathbf{b}-\text{proj}_{\text{CS}(A)}\mathbf{b}) \\
\stackrel{(\mathbf{b}-\text{proj}_{\text{CS}(A)}\mathbf{b})\in \text{CS}(A)^{\perp}}{\Rightarrow} &amp; A^t(\mathbf{b}-A\mathbf{x})=\mathbf{0} \\
\Rightarrow &amp; A^t A \mathbf{x}=A^t \mathbf{b}
\end{array}
\]
我個人使用的記憶法是<span style="color: red;">一稅在逼，a tax at b。</span>
<br>
如果 \(A\) 的column是linearly independent，則 \(A^t A\) 是invertible，於是可以得到 \(\mathbf{x}=(A^t A)^{-1} A^t \mathbf{b}\)。
<br>
<strong>Proof:</strong> We show that \(\text{N}(A^t A)=0\). Then by Dimension Theorem, \(\text{rank}(A^t A)=n\) and \(A^t A\) is invertible. 
\[
\begin{array}{cl}
\text{If} & (A^t A)\mathbf{x}=A^t(A\mathbf{x})=\mathbf{0} \\
\Rightarrow & A\mathbf{x}\in N(A^t)\cap \text{CS}(A) \\
\stackrel{\text{N}(A^t)\perp \text{CS}(A)}{\Rightarrow} & A\mathbf{x}=\mathbf{0} \\
\stackrel{\text{the columns of }A\text{ are linearly independent}}{\Rightarrow} & \mathbf{x}=\mathbf{0}
\end{array}
\]
</li>

<li>Anton, thm.6.4.6. Let \(A=QR\) be a \(QR\)-decomposition of \(A\). Then \(\mathbf{x}=R^{-1}Q^t\mathbf{b}\). 
<br>
<strong>Proof:</strong> This follows immediately from \(\mathbf{x}=(A^t A)^{-1}A^t \mathbf{b}\) and \(A=QR\).
</li>

</ol>


<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="singularvaluedecomposition">
Singular Value Decomposition</h2>


<p>
<font color="red">
<strong>Mnemonic: </strong>
</font>
\(\mathbf{v}_i\) are orthonormal basis for \(F^n\) consisting of eigenvectors of \(A^* A\) and \(\lambda_i\) are eigenvalues for \(A^* A\) and \(\text{rank }A=r\).
\[
\begin{array}{ccccc}
\left(\frac{A\mathbf{v}_1}{||A\mathbf{v}_1||}|\frac{A\mathbf{v}_2}{||A\mathbf{v}_2||}|\cdots  \right) & \begin{pmatrix}||A\mathbf{v}_1||=\sqrt{\lambda_1}\\ & ||A\mathbf{v}_2||=\sqrt{\lambda_2} \\ & & \ddots \\ &&& ||A\mathbf{v}_r||=\sqrt{\lambda_r} \\ &&&& 0 \\ &&&&& \ddots \\ &&&&&& 0 \end{pmatrix} & = & A & (\mathbf{v}_1|\mathbf{v}_2|\cdots |\mathbf{v}_n) \\
U_{m\times m} & \Sigma_{m\times n} & = & A_{m\times n} & V_{n\times n}
\end{array}
\]

</p>

<ul>
<li>Anton, sec.9.4.</li>
<li>Friedberg, thm.9.26</li>
<li>Friedberg, thm.9.27</li>
<li>Anton Contemporary, sec.8.6</li>
</ul>

<!---------------------------------->
<!---------------------------------->
<!---------------------------------->

<h2 id="pseudoinverse">
Pseudoinverse</h2>

<ul>
<li>Friedberg, p.413, def.</li>
<li>Friedberg, thm.6.29: \(A=U\Sigma V^*, A^{\dagger}=V\Sigma^{\dagger}U^*\)</li>
<li>Friedberg, thm.6.30(b): \(A\mathbf{x}=\mathbf{b}, \hat{\mathbf{x}}=A^{\dagger}\mathbf{b}\)</li>
</ul>




</body>


</html>